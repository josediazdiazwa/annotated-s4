{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636ba61e",
   "metadata": {},
   "source": [
    "# S4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9f3db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Efficiently Modeling Long Sequences with Structured State Spaces\n",
    "https://arxiv.org/abs/2111.00396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1423142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fe3c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import optax\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from jax.numpy.linalg import eig, inv\n",
    "from jax.numpy.linalg import matrix_power as power\n",
    "from jax.scipy.signal import convolve\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CONSTANTS (OPTIONS)\n",
    "MODEL = \"ff\"  # Option in < ff | lstm | s4-naive | s4-opt >\n",
    "DATASET = \"sin-ax+b\"  # Option in < sin-x | sin-ax+b | mnist >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem Definition\n",
    "if DATASET == \"sin-x\":\n",
    "    # Constants\n",
    "    SEQ_LENGTH, N_CLASSES = 360, 8\n",
    "\n",
    "    # Synthetic & Toy Experiment --> Overfit to a 8-bit quantized sin(x) from 0 - 2*Pi -- sampled 360 times\n",
    "    #   =>> Note: The Feed-Forward model won't necessarily be able to fit this data (optimization is hard)\n",
    "    #             As a sanity check, you can try running with N_CLASSES = 2 (-1, 1) and d_model = 1...\n",
    "    #             this is the simplest \"majority rule\" experiment --> gets 100% test accuracy.\n",
    "    print(\"[*] Generating sin(x) Toy Dataset...\")\n",
    "    x = onp.linspace(0, 2 * onp.pi, num=SEQ_LENGTH)\n",
    "    y = onp.digitize(onp.sin(x), onp.linspace(-1, 1, num=N_CLASSES))\n",
    "\n",
    "    # Tile this 1024 times (10 batches)...\n",
    "    data = torch.Tensor(\n",
    "        onp.tile(onp.expand_dims(onp.expand_dims(y, -1), 0), reps=[1024, 1, 1])\n",
    "    )\n",
    "\n",
    "    # Build Datasets -- Two entries to match MNIST (inputs, targets) structure...\n",
    "    trainset, testset = TensorDataset(data, data), TensorDataset(data[:1], data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif DATASET == \"sin-ax+b\":\n",
    "    # Constants\n",
    "    SEQ_LENGTH, N_CLASSES = 360, 8\n",
    "\n",
    "    # 8-bit quantized sin(ax + b) where `a` controls amplitude and `b` controls phase -- sampled 360 times\n",
    "    print(\"[*] Generating sin(ax + b) Dataset...\")\n",
    "    x = onp.linspace(0, 2 * onp.pi, num=SEQ_LENGTH)\n",
    "\n",
    "    # Generate 20K Instances -- `a` sampled uniform from [1, 10], `b` sampled uniform [0, 5]\n",
    "    N, A_MAX, B_MAX = 20000, 10, 5\n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    # Use Jax Randomness (no real reason, besides learning!)\n",
    "    data_key = jax.random.PRNGKey(21)\n",
    "    print(\"\\t=>> Generating 20K Training Examples...\")\n",
    "    for i in tqdm(range(N)):\n",
    "        data_key, a_rng, b_rng = jax.random.split(data_key, num=3)\n",
    "\n",
    "        # Compute a, b\n",
    "        a, b = jax.random.uniform(a_rng, minval=1.0, maxval=A_MAX), jax.random.uniform(\n",
    "            b_rng, maxval=B_MAX\n",
    "        )\n",
    "        train_data.append(\n",
    "            onp.digitize(onp.sin(a * x + b), onp.linspace(-1, 1, num=N_CLASSES))\n",
    "        )\n",
    "\n",
    "    # Test Data (1 Batch)\n",
    "    print(\"\\t=>> Generating 128 Test Examples...\")\n",
    "    for i in tqdm(range(128)):\n",
    "        data_key, a_rng, b_rng = jax.random.split(data_key, num=3)\n",
    "\n",
    "        # Compute a, b\n",
    "        a, b = jax.random.uniform(a_rng, minval=1.0, maxval=A_MAX), jax.random.uniform(\n",
    "            b_rng, maxval=B_MAX\n",
    "        )\n",
    "        test_data.append(\n",
    "            onp.digitize(onp.sin(a * x + b), onp.linspace(-1, 1, num=N_CLASSES))\n",
    "        )\n",
    "\n",
    "    # Build Datasets -- Two entries to match MNIST (inputs, targets) structure...\n",
    "    train_data = torch.Tensor(onp.expand_dims(onp.array(train_data), -1))\n",
    "    test_data = torch.Tensor(onp.expand_dims(onp.array(test_data), -1))\n",
    "    trainset, testset = TensorDataset(train_data, train_data), TensorDataset(\n",
    "        test_data, test_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif DATASET == \"mnist\":\n",
    "    # Constants\n",
    "    SEQ_LENGTH, N_CLASSES = 784, 256\n",
    "\n",
    "    # MNIST Sequence Modeling --> Predict next pixel value from history (autoregressively)\n",
    "    print(\"[*] Generating MNIST Sequence Modeling Dataset...\")\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.view(1, 784).t()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    testset = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    raise NotImplementedError(f\"Dataset `{DATASET}` not yet implemented...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7beda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data loaders, with fixed batch size\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=0\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=128, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91715f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Skeleton for S4 --> takes an S4Layer (naive/without-optimization, or fully loaded)\n",
    "class S4Model(nn.Module):\n",
    "    layer: nn.Module\n",
    "    d_output: int = 256\n",
    "    d_model: int = 64\n",
    "    n_layers: int = 4\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = nn.Dense(self.d_model)\n",
    "        layers, norms, dropouts = [], [], []\n",
    "        for _ in range(self.n_layers):\n",
    "            layers.append(self.layer)\n",
    "            norms.append(nn.LayerNorm())\n",
    "            dropouts.append(nn.Dropout(self.dropout, deterministic=False))\n",
    "        self.layers, self.norms, self.dropouts = layers, norms, dropouts\n",
    "        self.decoder = nn.Dense(self.d_output)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        def run(x):\n",
    "            x = self.encoder(x)\n",
    "            x = x.T\n",
    "            for layer, norm, dropout in zip(self.layers, self.norms, self.dropouts):\n",
    "                z = x\n",
    "                z = layer(z)\n",
    "                z = dropout(z)\n",
    "                x = z + x\n",
    "                x = norm(x.T).T\n",
    "            x = x.T\n",
    "            x = self.decoder(x)\n",
    "            x = nn.log_softmax(x)\n",
    "            return x\n",
    "\n",
    "        return jax.vmap(run)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55676ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General skeleton for a feed-forward model --> eventually should reconcile with above?\n",
    "class FeedForwardModel(nn.Module):\n",
    "    d_output: int = 256\n",
    "    d_model: int = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.d_model)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.d_output)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf38318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric definitions\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels[..., 0], num_classes=N_CLASSES)\n",
    "    return -np.mean(np.sum(one_hot_labels * logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    return np.mean(np.argmax(logits, -1) == labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc562418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set randomness...\n",
    "print(\"[*] Setting Randomness...\")\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, init_rng, dropout_rng = jax.random.split(key, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d641d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training state...\n",
    "def create_train_state(model, init_rng, dropout_rng):\n",
    "    params = model.init(\n",
    "        {\"params\": init_rng, \"dropout\": dropout_rng},\n",
    "        np.ones((128, SEQ_LENGTH - 1, 1)),\n",
    "    )[\"params\"]\n",
    "    tx = optax.adamw(1e-3)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c479b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S4 training loop (@Sidd - Reconcile all loops if possible?)\n",
    "def s4_train_step(state, batch, model):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply(\n",
    "            {\"params\": params}, batch[:, :-1], rngs={\"dropout\": dropout_rng}\n",
    "        )\n",
    "        loss = cross_entropy_loss(\n",
    "            logits=logits[:, : batch.shape[1]], labels=batch[:, 1:]\n",
    "        )\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Training Loop --> Jax by default can't JIT a \"nn.Module\" (it's immutable anyway), so ignore\n",
    "#   Note: Removing this jit call changes runtime from 0:06 --> 1:29 for an epoch on MNIST on a Titan RTX\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def ff_train_step(state, batch, model):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, batch[:, :-1])\n",
    "        loss = np.mean(jax.vmap(cross_entropy_loss)(logits, batch[:, 1:]))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53421043",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def ff_eval_step(batch, params, model):\n",
    "    logits = model.apply({\"params\": params}, batch[:, :-1])\n",
    "    loss = np.mean(jax.vmap(cross_entropy_loss)(logits, batch[:, 1:]))\n",
    "    acc = np.mean(jax.vmap(compute_accuracy)(logits, batch[:, 1:]))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31181b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core training loop\n",
    "def train_epoch(state, model):\n",
    "    # Set train_step function...\n",
    "    if MODEL == \"ff\":\n",
    "        train_step = ff_train_step\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Training step for model `{MODEL}` not yet implemented...\"\n",
    "        )\n",
    "\n",
    "    # Store Metrics\n",
    "    batch_losses = []\n",
    "    for batch_idx, (inputs, _) in enumerate(tqdm(trainloader)):\n",
    "        inputs = np.array(inputs.numpy())\n",
    "        state, loss = train_step(state, inputs, model)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "    # Return average loss over batches\n",
    "    return np.mean(np.array(batch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da46616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function for MNIST dataset\n",
    "def validate_mnist(params, model):\n",
    "    # Set train_step function...\n",
    "    if MODEL == \"ff\":\n",
    "        eval_step = ff_eval_step\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Training step for model `{MODEL}` not yet implemented...\"\n",
    "        )\n",
    "\n",
    "    # Compute average loss & accuracy\n",
    "    losses, accuracies = [], []\n",
    "    for batch_idx, (inputs, _) in enumerate(tqdm(testloader)):\n",
    "        inputs = np.array(inputs.numpy())\n",
    "        loss, acc = eval_step(inputs, params, model)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    # Sampling MNIST digits starting prompted w/ first 100 \"tokens\"...\n",
    "    #   => TODO @Sidd\n",
    "\n",
    "    return np.mean(np.array(losses)), np.mean(np.array(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc044fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "print(\"[*] Starting Training =>> Initializing Model + Train State...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4051bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch on model type\n",
    "if MODEL == \"ff\":\n",
    "    model = FeedForwardModel(d_output=N_CLASSES)\n",
    "elif MODEL == \"lstm\":\n",
    "    # => TODO @Sidd\n",
    "    raise NotImplementedError(\"LSTM/Recurrent model not yet implemented...\")\n",
    "elif MODEL == \"s4-naive\":\n",
    "    # => TODO -- Need to move block down...\n",
    "    # model = S4Model(layer=NaiveS4Layer)\n",
    "    pass\n",
    "elif MODEL == \"s4-opt\":\n",
    "    # => TODO -- Need to move block down...\n",
    "    # model = S4Model(layer=OptimizedS4Layer)\n",
    "    pass\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model `{MODEL}` not yet implemented...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boilerplate train state and jump into training\n",
    "state = create_train_state(model, init_rng, dropout_rng)\n",
    "for epoch in range(10):\n",
    "    print(f\"[*] Starting Training Epoch {epoch + 1}...\")\n",
    "    train_loss = train_epoch(state, model)\n",
    "\n",
    "    print(f\"[*] Running Epoch {epoch + 1} Validation...\")\n",
    "    test_loss, test_acc = validate_mnist(state.params, model)\n",
    "\n",
    "    print(f\"\\n=>> Epoch {epoch + 1} Metrics ===\")\n",
    "    print(\n",
    "        f\"\\tTrain Loss: {train_loss:.5f} -- Test Loss: {test_loss:.5f} -- Test\"\n",
    "        f\" Accuracy: {test_acc:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed975f",
   "metadata": {},
   "source": [
    "model = MNistModel(Layer)\n",
    "train_epoch(create_train_state(model, init_rng, dropout_rng, 0.1, 0.01), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42098dbc",
   "metadata": {},
   "source": [
    "# Part 1: The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e6f38",
   "metadata": {},
   "source": [
    "The state space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6769a",
   "metadata": {},
   "source": [
    "## State Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4412851",
   "metadata": {},
   "source": [
    "Create a state space model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada63524",
   "metadata": {},
   "source": [
    "A state space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669d466",
   "metadata": {},
   "source": [
    "$x'(t) = \\mathbf{A} x(t) + \\mathbf{B} u(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88424351",
   "metadata": {},
   "source": [
    "$y(t) = \\mathbf{C} x(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972f006",
   "metadata": {},
   "source": [
    "To simplify everything we will assume the following.\n",
    "$\\mathbf{A} \\in R^{N \\times N}, \\mathbf{B} \\in R^{N \\times 1}, \\mathbf{C} \\in R^{1 \\times N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c5070",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "and $u(t) : R \\mapsto R$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff36eac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Discretization to recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c966f",
   "metadata": {},
   "source": [
    "Discretize SSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b245417",
   "metadata": {},
   "source": [
    "Bilinear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a4464",
   "metadata": {},
   "source": [
    "important todo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5992ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "https://en.wikipedia.org/wiki/Bilinear_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdb574",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def discretize_SSM(A, B, C, step=1):\n",
    "    I = np.eye(A.shape[0])\n",
    "    BL = inv((I - (step / 2.0) * A))\n",
    "    Abar = BL @ (I + (step / 2.0) * A)\n",
    "    Bbar = (BL * step) @ B\n",
    "    return Abar, Bbar, C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562f3fb",
   "metadata": {},
   "source": [
    "This allows us to run the model as a recurrent network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f9ad9",
   "metadata": {},
   "source": [
    "$x_{t+1} = \\mathbf{\\bar{A}} x_t + \\mathbf{\\bar{B}} u_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3971e",
   "metadata": {},
   "source": [
    "$y_t = \\mathbf{\\bar{C}} x_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e50d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7ad1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def iterative_SSM(A, B, C, y):\n",
    "    def f(X, y):\n",
    "        X = A @ X + (B * y).ravel()\n",
    "        return X, C @ X\n",
    "\n",
    "    return jax.lax.scan(f, np.zeros(B.shape[0]), y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9604163",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ac0b9",
   "metadata": {},
   "source": [
    "Because each step of the SSM is linear we can\n",
    "also compute $y_t$ without a recurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe2596",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f76b7",
   "metadata": {},
   "source": [
    "Make a conv filter version of SSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe1ffb",
   "metadata": {},
   "source": [
    "$K = (\\bar{C} \\bar{B}, \\bar{C} \\bar{A}^1 \\bar{B}, \\ldots, \\bar{C} \\bar{A}^{L-1} \\bar{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ded279",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We call $K \\in R^L$ the convolutional filter representation of the discrete SSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841cdd5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def K_conv(A, B, C, L):\n",
    "    return np.array([(C @ power(A, l) @ B).reshape() for l in range(L)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee050726",
   "metadata": {},
   "source": [
    "We can then compute $y$ by convolution with this $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54320f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This convolution is big though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb5cbd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def nonCircularConvolution(x, filt):\n",
    "    return convolve(x, filt, mode=\"full\", method=\"fft\")[: x.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b1515",
   "metadata": {},
   "source": [
    "## HiPPO matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130baf9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "For this model to work, initialization is really important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ba3c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_HiPPO(N):\n",
    "    p = np.sqrt(2 * np.arange(1, N + 1) + 1.0)\n",
    "    A = p[:, np.newaxis] @ p[np.newaxis, :]\n",
    "    return -np.tril(A, k=-1) - np.diag(np.arange(1, N + 1) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb62e78",
   "metadata": {},
   "source": [
    "## Running it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4756f23",
   "metadata": {},
   "source": [
    "Create a model\n",
    "N1 = 16\n",
    "L1 = 64\n",
    "LR = 0.1\n",
    "A, B, C = make_HiPPO(N1), Param((N1, 1)), Param((1, N1))\n",
    "params = [B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3f87e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "def model(params, y):\n",
    "    ssm = discretize_SSM(A, params[0], params[1])\n",
    "    # Turn to convolution\n",
    "    K = K_conv(*ssm, L1)\n",
    "    # Run as convolution\n",
    "    return nonCircularConvolution(y, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d63149",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class NaiveS4Layer(nn.Module):\n",
    "    H: int = 50\n",
    "    l_max: int = 16\n",
    "    dropout: float = 0.2\n",
    "    N: int = 50\n",
    "\n",
    "    def setup(self):\n",
    "        self.A = make_HiPPO(self.N).reshape(1, self.N, self.N)\n",
    "        self.B = self.param(\"B\", nn.initializers.zeros, (self.H, self.N, 1))\n",
    "        self.C = self.param(\"C\", nn.initializers.zeros, (self.H, 1, self.N))\n",
    "\n",
    "    def __call__(self, y):\n",
    "        def create_ssms(A, B, C):\n",
    "            ssm = discretize_SSM(A, B, C)\n",
    "            return K_conv(*ssm, self.l_max)\n",
    "\n",
    "        K = jax.vmap(lambda B, C: create_ssms(self.A, B, C))(self.B, self.C)\n",
    "\n",
    "        # Run as convolution\n",
    "        return jax.vmap(nonCircularConvolution)(y, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569c4b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "def run_train():\n",
    "\n",
    "    # Run the ssm\n",
    "    ssm = discretize_SSM(A, params[0], params[1])\n",
    "    v = iterative_SSM(*ssm, y)\n",
    "\n",
    "    plt.plot(x[1:65], v[1:])\n",
    "    plt.plot(x[1:65], y[1:])\n",
    "    __st.pyplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff0bdd",
   "metadata": {},
   "source": [
    "# Part 2: Doing it Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80b410",
   "metadata": {},
   "source": [
    "## Generating Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b8ea3",
   "metadata": {},
   "source": [
    "The key idea that S4 is going to exploit is generating functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bf71a",
   "metadata": {},
   "source": [
    "In particular we are going to convert $K$ from a convolution filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37872a",
   "metadata": {},
   "source": [
    "$K = (\\bar{C} \\bar{B}, \\bar{C} \\bar{A}^1 \\bar{B}, \\ldots, \\bar{C} \\bar{A}^{L-1} \\bar{B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07fcfc5",
   "metadata": {},
   "source": [
    "Into a polynomial where each coefficient represents one element of this sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fde6d7",
   "metadata": {},
   "source": [
    "discSSM = discretize_SSM(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b725d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# $\\hat{K}(z) = \\bar{C} \\bar{B}   + \\bar{C} \\bar{A}^1 \\bar{B} z^1 + \\ldots + \\bar{C} \\bar{A}^{L-1} \\bar{B}  z^{L-1}$\n",
    "def K_gen_simple(*ssm, L):\n",
    "    K = K_conv(*ssm, L)\n",
    "    return lambda z: np.sum(K * (z ** np.arange(L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cc1f5",
   "metadata": {},
   "source": [
    "If we apply this function at specific values, we can get back the original convolutional filter.\n",
    "\n",
    "In particular we apply at the roots of unity,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eaf858",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "$$\\Omega_L = \\{\\exp(2 \\pi i  \\frac{k}{M}) : k \\in [L])\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d790c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "And then take an inverse fourier transform to get them back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc063b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def convFromGen(gen, L):\n",
    "    r = np.exp((2j * np.pi / L) * np.arange(L))\n",
    "    atRoots = jax.vmap(gen)(r)\n",
    "    order = np.array([i if i == 0 else L - i for i in range(L)])\n",
    "    out = np.fft.ifft(atRoots, L).reshape(L)\n",
    "    return out[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e1946",
   "metadata": {},
   "source": [
    "What was the point of that? Well working with the generating\n",
    "function allows us to do some algebraic manipulations to\n",
    "reduce elimanate some of the hard terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e5118",
   "metadata": {},
   "source": [
    "In particular the main trick we are going to apply is to turn\n",
    "the repeated exponentiation into an inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4bc00",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "$\\hat{K}(z) = \\bar{C} ( I - A^L) (I - A z)^{-1} \\bar{B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a904dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def K_gen_inverse(A, B, C, L):\n",
    "    I = np.eye(A.shape[0])\n",
    "    A_L = power(A, L)\n",
    "    C2 = C @ (I - A_L)\n",
    "    return lambda z: (C2 @ inv(I - A * z) @ B).reshape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1696c",
   "metadata": {},
   "source": [
    "K2 = convFromGen(K_gen_inverse(A, B, C, L=16), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a7a1c",
   "metadata": {},
   "source": [
    "K2.real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911d044",
   "metadata": {},
   "source": [
    "By working with this generating function we will be able to compute our main term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e40a4c",
   "metadata": {},
   "source": [
    "## Diagonal Plus Low Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e89e7",
   "metadata": {},
   "source": [
    "This generating function allows us to avoid the matrix power. However it replaces\n",
    "it with an inverse which is still not great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6de435",
   "metadata": {},
   "source": [
    "$(C2  (I - \\Gamma  z)^{-1}  B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea691ec",
   "metadata": {},
   "source": [
    "Let's imagine for a second though that $A$ was diagonal. Then you have a nice form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea1dad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "$\\sum_{i} \\frac{C_{1, i} B_{i, 1}} { {1 - \\Gamma_{ii} z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dea2a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Will make a simple function to compute sums of this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28559106",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def cauchy_dot(v, omega, lambd):\n",
    "    return (v / (omega - lambd)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc2df9",
   "metadata": {},
   "source": [
    "Diagonal is a pretty strong assumption. But we can relax it by allowing for\n",
    "a low-rank component as well with $p, q \\in C^{N\\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9e48c",
   "metadata": {},
   "source": [
    "$A = \\Gamma + p  q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da700456",
   "metadata": {},
   "source": [
    "The Woodbury identity tells us that the inverse of a diagonal plus low-rank\n",
    "is equal to a diagonal plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5495ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "https://en.wikipedia.org/wiki/Woodbury_matrix_identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291977",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "$(\\Gamma + p  q^*)^{-1} = \\Gamma^{-1} + \\Gamma^{-1} p (1 + p^* q)^-1 v^* \\Gamma^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32653c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The math to get there for real is a bit complex, but here is what the function looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_gen_DPLR(Gamma, p, q, B, Ct, step=1):\n",
    "    aterm = (Ct.conj().ravel(), q.conj().ravel())\n",
    "    bterm = (B.ravel(), p.ravel())\n",
    "\n",
    "    def gen(o):\n",
    "        f = (2.0 / step) * ((1.0 - o) / (1.0 + o))\n",
    "        k00 = cauchy_dot(aterm[0] * bterm[0], f, Gamma)\n",
    "        k01 = cauchy_dot(aterm[0] * bterm[1], f, Gamma)\n",
    "        k10 = cauchy_dot(aterm[1] * bterm[0], f, Gamma)\n",
    "        k11 = cauchy_dot(aterm[1] * bterm[1], f, Gamma)\n",
    "        return (2.0 / (1.0 + o)) * (k00 - k01 * (1.0 / (1.0 + k11)) * k10)\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dd74b",
   "metadata": {},
   "source": [
    "## Turning HiPPO to DPLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CPU asymmetric eigenvalue decomposition\n",
    "eig_cpu = jax.jit(eig, backend=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afd784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DPLR_HiPPO(N):\n",
    "    p = np.sqrt(2 * np.arange(1, N + 1) + 1.0)\n",
    "    q = p\n",
    "    A = p[:, np.newaxis] @ q[np.newaxis, :]\n",
    "    hippo = -np.tril(A, k=-1) - np.diag(np.arange(1, N + 1) + 1)\n",
    "    S = hippo + 0.5 * A + 0.5 * np.eye(N)\n",
    "\n",
    "    # Skew symmetric -- @Sidd Note: eig/eigvals not GPU/lax-backed, so call from cpu instead...\n",
    "    diag, v = eig_cpu(S)\n",
    "    diag -= 0.5\n",
    "\n",
    "    return hippo, diag, 0.5 * p, q, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbadad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ## The Model\n",
    "class OptimizedS4Layer(nn.Module):\n",
    "    H: int = 50\n",
    "    L: int = 16\n",
    "    N: int = 50\n",
    "    step: int = 1\n",
    "\n",
    "    def setup(self):\n",
    "        self.A, self.Gamma, self.p, self.q, _ = make_DPLR_HiPPO(self.N)\n",
    "        self.B = self.param(\"B\", nn.initializers.zeros, (self.H, self.N, 1))\n",
    "        self.C = self.param(\"C\", nn.initializers.zeros, (self.H, 1, self.N))\n",
    "\n",
    "        Abar, _, Cbar = discretize_SSM(self.A, self.B, self.C, self.step)\n",
    "        self.Ct = jax.vmap(\n",
    "            lambda Cbar: (np.eye(self.N) - power(Abar, self.L)).conj().T @ Cbar.ravel()\n",
    "        )(Cbar)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        def create_ssms(B, Ct):\n",
    "            K_gen = K_gen_DPLR(self.Gamma, self.p, self.q, B, Ct, self.step)\n",
    "            return convFromGen(K_gen, self.L)\n",
    "\n",
    "        K = jax.vmap(create_ssms)(self.B, self.Ct)\n",
    "        return jax.vmap(nonCircularConvolution)(y, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e933a",
   "metadata": {},
   "source": [
    "FULL CALL\n",
    "model = MNistModel(layer=S4(H=256, L=64))\n",
    "train_epoch(create_train_state(model, init_rng, dropout_rng, 0.1, 0.01), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b40a",
   "metadata": {},
   "source": [
    "# Part 3: Pathing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab2c93",
   "metadata": {},
   "source": [
    "## Path-X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d735c",
   "metadata": {},
   "source": [
    "L = 16000\n",
    "s = S4(L, 2)\n",
    "out2 = s.K_gen()\n",
    "out2 = convFromGen(out2, L)\n",
    "out2"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
